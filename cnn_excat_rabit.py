# -*- coding: utf-8 -*-
"""CNN_EXCat_Rabit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11yPwItMO_6w_ZJMpTXy69zqD8GIeEvtY
"""

!pip install roboflow

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch import optim
import matplotlib.pyplot as plt
from roboflow import Roboflow

rf = Roboflow(api_key="UinOpQSEUeMiovI9b24c")
project = rf.workspace("visiontuner-fpebk").project("classification-uowwo")
dataset = project.version(1).download("folder")

train_transform = transforms.Compose([
    transforms.Resize((300, 300)), #The Resize transform makes sure all images are of the same size.
    transforms.RandomRotation(10),     # RandomRotation, RandomHorizontalFlip, and RandomVerticalFlip introduce variations in the training data
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.ToTensor(), #transforms the images into PyTorch tensors
])

test_transform = transforms.Compose([
    transforms.Resize((300, 300)),
    transforms.ToTensor(),
])

train_dir = "/content/classification-1/train"
test_dir = "/content/classification-1/test"
val_dir = "/content/classification-1/valid"
#load the datasets using ImageFolder to organize the images into classes based on their folder names
#This makes it easier to handle the data during training
val_dataset = datasets.ImageFolder(val_dir, transform=test_transform)
train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)
test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=False)

#Print the shape of each image along with its label to understand how the data is organized and labeled across the datasets
print("Train Dataset:")
for i, (image, label) in enumerate(train_dataset):
    print(f"Sample {i + 1}: Image shape: {image.shape}, Label: {label}")

print("\nTest Dataset:")
for i, (image, label) in enumerate(test_dataset):
    print(f"Sample {i + 1}: Image shape: {image.shape}, Label: {label}")

print("\nValidation Dataset:")
for i, (image, label) in enumerate(val_dataset):
    print(f"Sample {i + 1}: Image shape: {image.shape}, Label: {label}")

torch.manual_seed(42)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(128 * 75 * 75, 256)  # Adjusted input size
        self.fc2 = nn.Linear(256, 2)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# I used the CrossEntropyLoss function to compute the loss and Adam as the optimizer
model = CNN().to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)

epochs = 13

train_losses = [] # I initialize empty lists to store training losses,valid losses and also for valid accuracy
valid_losses = []
valid_accuracy = []

for epoch in range(epochs):
  # I set the model to train mode to update its weights
    model.train()
    epoch_weighted_loss = 0

    # Here, it iterates over each batch in the training data.
    for batch_idx, (features, labels) in enumerate(train_loader):
        features, labels = features.to(device), labels.to(device)

        predictions = model(features) #Passing inputs to the model
        loss = loss_fn(predictions, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        #update the epoch weighted loss
        epoch_weighted_loss += loss.item() * len(features)

    #calculate the average
    epoch_loss = epoch_weighted_loss / len(train_loader.dataset)
    train_losses.append(epoch_loss)

    # Validation ,set the model to evaluation mode
    model.eval()
    valid_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for batch_idx, (features, labels) in enumerate(val_loader):
            features, labels = features.to(device), labels.to(device)
            predictions = model(features)
            valid_loss += loss_fn(predictions, labels).item()
            _, predicted = torch.max(predictions, 1)  # Convert the predicted probabilities to predicted class labels
            # Update the number of correct predictions and total predictions
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

    valid_losses.append(valid_loss / len(val_loader.dataset))
    valid_accuracy.append(correct / total)

    print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {epoch_loss:.4f}, Valid Loss: {valid_losses[-1]:.4f}, Valid Accuracy: {valid_accuracy[-1]:.4f}')

plt.plot(train_losses, label='Train Loss')
plt.plot(valid_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

correct = 0
total = 0
with torch.no_grad():
    for features, labels in test_loader:
        features, labels = features.to(device), labels.to(device)
        predictions = model(features)
        _, predicted_labels = torch.max(predictions, 1)  # Get the predicted class labels
        correct += (predicted_labels == labels).sum().item()
        total += labels.size(0)

# Calculate the accuracy by dividing the number of correct predictions by the total number of predictions
accuracy = (correct / total)*100
print(f"Accuracy: {accuracy}")